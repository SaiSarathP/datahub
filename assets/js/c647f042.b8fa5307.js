"use strict";(self.webpackChunkdocs_website=self.webpackChunkdocs_website||[]).push([[153],{4137:function(e,t,a){a.d(t,{Zo:function(){return h},kt:function(){return p}});var o=a(7294);function n(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,o)}return a}function r(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){n(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,o,n=function(e,t){if(null==e)return{};var a,o,n={},i=Object.keys(e);for(o=0;o<i.length;o++)a=i[o],t.indexOf(a)>=0||(n[a]=e[a]);return n}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(o=0;o<i.length;o++)a=i[o],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(n[a]=e[a])}return n}var l=o.createContext({}),d=function(e){var t=o.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):r(r({},t),e)),a},h=function(e){var t=d(e.components);return o.createElement(l.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},c=o.forwardRef((function(e,t){var a=e.components,n=e.mdxType,i=e.originalType,l=e.parentName,h=s(e,["components","mdxType","originalType","parentName"]),c=d(a),p=n,m=c["".concat(l,".").concat(p)]||c[p]||u[p]||i;return a?o.createElement(m,r(r({ref:t},h),{},{components:a})):o.createElement(m,r({ref:t},h))}));function p(e,t){var a=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var i=a.length,r=new Array(i);r[0]=c;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:n,r[1]=s;for(var d=2;d<i;d++)r[d]=a[d];return o.createElement.apply(null,r)}return o.createElement.apply(null,a)}c.displayName="MDXCreateElement"},6571:function(e,t,a){a.r(t),a.d(t,{frontMatter:function(){return s},contentTitle:function(){return l},metadata:function(){return d},toc:function(){return h},default:function(){return c}});var o=a(7462),n=a(3366),i=(a(7294),a(4137)),r=["components"],s={title:"DataHub FAQs",sidebar_label:"FAQs",slug:"/faq",custom_edit_url:"https://github.com/linkedin/datahub/blob/master/docs/faq.md"},l="DataHub FAQs",d={unversionedId:"docs/faq",id:"docs/faq",isDocsHomePage:!1,title:"DataHub FAQs",description:"Why should we use DataHub?",source:"@site/genDocs/docs/faq.md",sourceDirName:"docs",slug:"/faq",permalink:"/docs/faq",editUrl:"https://github.com/linkedin/datahub/blob/master/docs/faq.md",tags:[],version:"current",frontMatter:{title:"DataHub FAQs",sidebar_label:"FAQs",slug:"/faq",custom_edit_url:"https://github.com/linkedin/datahub/blob/master/docs/faq.md"}},h=[{value:"Why should we use DataHub?",id:"why-should-we-use-datahub",children:[],level:2},{value:"Would you recommend DataHub rather than available commercial solutions?",id:"would-you-recommend-datahub-rather-than-available-commercial-solutions",children:[],level:2},{value:"Who are the major contributors in the community?",id:"who-are-the-major-contributors-in-the-community",children:[],level:2},{value:"How big is the community?",id:"how-big-is-the-community",children:[],level:2},{value:"Is there a contributor selection criteria?",id:"is-there-a-contributor-selection-criteria",children:[],level:2},{value:"How does the project engage with the community?",id:"how-does-the-project-engage-with-the-community",children:[],level:2},{value:"Should this be the platform we decide upon, we\u2019d like to fully engage and work with LinkedIn and the community. What\u2019s the best way and what level of engagement/involvement should we expect?",id:"should-this-be-the-platform-we-decide-upon-wed-like-to-fully-engage-and-work-with-linkedin-and-the-community-whats-the-best-way-and-what-level-of-engagementinvolvement-should-we-expect",children:[],level:2},{value:"What\u2019s the best way to ramp up the product knowledge to properly test and evaluate DataHub?",id:"whats-the-best-way-to-ramp-up-the-product-knowledge-to-properly-test-and-evaluate-datahub",children:[],level:2},{value:"Where can I learn about the roadmap?",id:"where-can-i-learn-about-the-roadmap",children:[],level:2},{value:"Where can I learn about the current list of features/functionalities?",id:"where-can-i-learn-about-the-current-list-of-featuresfunctionalities",children:[],level:2},{value:"Are the product strategy/vision/roadmap driven by the LinkedIn Engineering team, community, or a collaborative effort?",id:"are-the-product-strategyvisionroadmap-driven-by-the-linkedin-engineering-team-community-or-a-collaborative-effort",children:[],level:2},{value:"Does DataHub connect with Google Cloud Platform?",id:"does-datahub-connect-with-google-cloud-platform",children:[],level:2},{value:"How approachable would LinkedIn be to provide insights/support or collaborate on a functionality?",id:"how-approachable-would-linkedin-be-to-provide-insightssupport-or-collaborate-on-a-functionality",children:[],level:2},{value:"How do LinkedIn Engineering team and the community ensure the quality of the community code for DataHub?",id:"how-do-linkedin-engineering-team-and-the-community-ensure-the-quality-of-the-community-code-for-datahub",children:[],level:2},{value:"How are the ingestion ETL processes scheduled at LinkedIn?",id:"how-are-the-ingestion-etl-processes-scheduled-at-linkedin",children:[],level:2},{value:"What are the options for the Kafka Key for MCE, MAE and FailedMCE Topic?",id:"what-are-the-options-for-the-kafka-key-for-mce-mae-and-failedmce-topic",children:[],level:2},{value:"How is the Data Quality of the ingestion messages handled?",id:"how-is-the-data-quality-of-the-ingestion-messages-handled",children:[],level:2},{value:"Can you give a high level overview about how Data Governance is handled? What privacy/governance use cases are supported in LinkedIn through DataHub?",id:"can-you-give-a-high-level-overview-about-how-data-governance-is-handled-what-privacygovernance-use-cases-are-supported-in-linkedin-through-datahub",children:[],level:2},{value:"When using Kafka and Confluent Schema Registry, does DataHub support multiple schemas for the same topic?",id:"when-using-kafka-and-confluent-schema-registry-does-datahub-support-multiple-schemas-for-the-same-topic",children:[],level:2},{value:"How do we better document and map transformations within an ETL process? How do we create institutional knowledge and processes to help create a paradigm for tribal knowledge?",id:"how-do-we-better-document-and-map-transformations-within-an-etl-process-how-do-we-create-institutional-knowledge-and-processes-to-help-create-a-paradigm-for-tribal-knowledge",children:[],level:2},{value:"How do we advance the product from a Data Catalog Browser to a Data Collaboration environment like Alation?",id:"how-do-we-advance-the-product-from-a-data-catalog-browser-to-a-data-collaboration-environment-like-alation",children:[],level:2},{value:"Can you share how the catalog looks in LinkedIn production?",id:"can-you-share-how-the-catalog-looks-in-linkedin-production",children:[],level:2},{value:"Does the roadmap have provision for capturing the Data Quality Information of the Dataset?",id:"does-the-roadmap-have-provision-for-capturing-the-data-quality-information-of-the-dataset",children:[],level:2},{value:"Is DataHub capturing/showing column level constraints set at table definition?",id:"is-datahub-capturingshowing-column-level-constraints-set-at-table-definition",children:[],level:2},{value:"How does DataHub manage extracting metadata from stores residing in different security zones?",id:"how-does-datahub-manage-extracting-metadata-from-stores-residing-in-different-security-zones",children:[],level:2},{value:"What all data stores does DataHub backend support presently?",id:"what-all-data-stores-does-datahub-backend-support-presently",children:[],level:2},{value:"For which stores, you have discovery services?",id:"for-which-stores-you-have-discovery-services",children:[],level:2},{value:"How is metadata ingested in DataHub? Is it real-time?",id:"how-is-metadata-ingested-in-datahub-is-it-real-time",children:[],level:2}],u={toc:h};function c(e){var t=e.components,a=(0,n.Z)(e,r);return(0,i.kt)("wrapper",(0,o.Z)({},u,a,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"datahub-faqs"},"DataHub FAQs"),(0,i.kt)("h2",{id:"why-should-we-use-datahub"},"Why should we use DataHub?"),(0,i.kt)("p",null,"DataHub is a self-service data portal which provides search and discovery capabilities (and more!) over the data assets of an organization. This tool can help improve productivity of data scientists, data analysts, engineers of organizations dealing with massive amounts of data. Also, the regulatory environment (GDPR, CCPA etc) requires a company to know what data it has, who is using it and how long it will be retained - DataHub provides a solution to these challenges by gathering metadata across a distributed data ecosystem and surfacing it as a data catalog thereby easing the burden of data privacy/compliance."),(0,i.kt)("h2",{id:"would-you-recommend-datahub-rather-than-available-commercial-solutions"},"Would you recommend DataHub rather than available commercial solutions?"),(0,i.kt)("p",null,"Common problems with commercial solutions can be summarized as:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Lacks direct access to source code: Any feature gaps can only be closed by external parties, which can be both time consuming and expensive."),(0,i.kt)("li",{parentName:"ul"},"Dependency on larger proprietary systems or environments, e.g. AWS, Azure, Cloudera etc., making it infeasible to adopt if it doesn\u2019t fit your environment."),(0,i.kt)("li",{parentName:"ul"},"Expensive to acquire, integrate and operate"),(0,i.kt)("li",{parentName:"ul"},"Vendor Lock-in")),(0,i.kt)("p",null,"DataHub can be right for you if you want an open source unbundled solution (front-end application completely decoupled from a \u201cbattle-tested\u201d metadata store), that you are free to modify, extend and integrate with your data ecosystem. In our experience at LinkedIn and talking to other companies in a similar situation, metadata always has a very company specific implementation and meaning. Commercial tools will typically drop-in and solve a few use-cases well out of the gate, but will need much more investment or will be impossible to extend for some specific kinds of metadata."),(0,i.kt)("h2",{id:"who-are-the-major-contributors-in-the-community"},"Who are the major contributors in the community?"),(0,i.kt)("p",null,"Currently LinkedIn engineers. However, we\u2019re receiving more and more PRs from individuals working at various companies."),(0,i.kt)("h2",{id:"how-big-is-the-community"},"How big is the community?"),(0,i.kt)("p",null,"Check out our adoption ",(0,i.kt)("a",{parentName:"p",href:"/docs/#adoption"},"here")),(0,i.kt)("h2",{id:"is-there-a-contributor-selection-criteria"},"Is there a contributor selection criteria?"),(0,i.kt)("p",null,"We welcome contributions from everyone in the community. Please read our ",(0,i.kt)("a",{parentName:"p",href:"/docs/contributing"},"contributing guidelines"),". In general, we will review PRs with the same rigor as our internal code review process to maintain overall quality."),(0,i.kt)("h2",{id:"how-does-the-project-engage-with-the-community"},"How does the project engage with the community?"),(0,i.kt)("p",null,"We organize public town hall meetings at a monthly cadence. We use ",(0,i.kt)("a",{parentName:"p",href:"https://slack.datahubproject.io"},"Slack")," as one of the main ways to support the community."),(0,i.kt)("h2",{id:"should-this-be-the-platform-we-decide-upon-wed-like-to-fully-engage-and-work-with-linkedin-and-the-community-whats-the-best-way-and-what-level-of-engagementinvolvement-should-we-expect"},"Should this be the platform we decide upon, we\u2019d like to fully engage and work with LinkedIn and the community. What\u2019s the best way and what level of engagement/involvement should we expect?"),(0,i.kt)("p",null,"The best way to engage is through the ",(0,i.kt)("a",{parentName:"p",href:"https://slack.datahubproject.io"},"Slack channel"),". You\u2019ll get to interact with the developers and the community. It is a vibrant community and most questions are answered within a few hours by the community. "),(0,i.kt)("p",null,"For reproducible technical issues, bugs and code contributions, Github ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/linkedin/datahub/issues"},"issues")," and ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/linkedin/datahub/pulls"},"PRs")," are the preferred channel."),(0,i.kt)("h2",{id:"whats-the-best-way-to-ramp-up-the-product-knowledge-to-properly-test-and-evaluate-datahub"},"What\u2019s the best way to ramp up the product knowledge to properly test and evaluate DataHub?"),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"/docs"},"The docs")," are the best resource. We have documented the steps to install and test DataHub thoroughly there."),(0,i.kt)("p",null,"The ",(0,i.kt)("a",{parentName:"p",href:"https://engineering.linkedin.com/blog/2019/data-hub"},"DataHub Introduction")," and ",(0,i.kt)("a",{parentName:"p",href:"https://engineering.linkedin.com/blog/2020/open-sourcing-datahub--linkedins-metadata-search-and-discovery-p"},"Open Sourcing Datahub")," blog posts are also useful resources for getting a high level understanding of the system."),(0,i.kt)("h2",{id:"where-can-i-learn-about-the-roadmap"},"Where can I learn about the roadmap?"),(0,i.kt)("p",null,"You can learn more about DataHub's ",(0,i.kt)("a",{parentName:"p",href:"/docs/roadmap"},"product roadmap"),", which gets updated regularly."),(0,i.kt)("h2",{id:"where-can-i-learn-about-the-current-list-of-featuresfunctionalities"},"Where can I learn about the current list of features/functionalities?"),(0,i.kt)("p",null,"You can learn more about the current ",(0,i.kt)("a",{parentName:"p",href:"/docs/features"},"list of features"),"."),(0,i.kt)("h2",{id:"are-the-product-strategyvisionroadmap-driven-by-the-linkedin-engineering-team-community-or-a-collaborative-effort"},"Are the product strategy/vision/roadmap driven by the LinkedIn Engineering team, community, or a collaborative effort?"),(0,i.kt)("p",null,"Mixed of both LinkedIn DataHub team and the community. The roadmap will be a joint effort of both LinkedIn and the community. However, we\u2019ll most likely prioritize tasks that align with the community's asks."),(0,i.kt)("h2",{id:"does-datahub-connect-with-google-cloud-platform"},"Does DataHub connect with Google Cloud Platform?"),(0,i.kt)("p",null,"LinkedIn is not using GCP so we cannot commit to building and testing that connectivity. However, we\u2019ll be happy to accept community contributions for GCP integration. Also, our Slack channel and regularly scheduled town hall meetings are a good opportunity to meet with people from different companies who have similar requirements and might be interested in collaborating on these features."),(0,i.kt)("h2",{id:"how-approachable-would-linkedin-be-to-provide-insightssupport-or-collaborate-on-a-functionality"},"How approachable would LinkedIn be to provide insights/support or collaborate on a functionality?"),(0,i.kt)("p",null,"Please take a look at our ",(0,i.kt)("a",{parentName:"p",href:"/docs/roadmap"},"roadmap")," & ",(0,i.kt)("a",{parentName:"p",href:"/docs/features"},"features")," to get a sense of what\u2019s being open sourced in the near future. If there\u2019s something missing from the list, we\u2019re open to discussion. In fact, the town hall would be the perfect venue for such discussions."),(0,i.kt)("h2",{id:"how-do-linkedin-engineering-team-and-the-community-ensure-the-quality-of-the-community-code-for-datahub"},"How do LinkedIn Engineering team and the community ensure the quality of the community code for DataHub?"),(0,i.kt)("p",null,"All PRs are reviewed by the LinkedIn team. Any extension/contribution coming from the community which LinkedIn team doesn\u2019t have any expertise on will be placed into a incuation directory first (",(0,i.kt)("inlineCode",{parentName:"p"},"/contrib"),"). Once it\u2019s blessed and adopted by the community, we\u2019ll graduate it from incubation and move it into the main code base."),(0,i.kt)("p",null,"In the beginning, LinkedIn will play a big role in terms of stewardship of the code base. We\u2019ll re-evaluate this strategy based on the amount of engagement from the community. There is a large backlog of features that we have only for the internal DataHub. We are going through the effort of generalizing and open sourcing these features. This will lead to batches of large commits from LinkedIn in the near future until the two code bases get closely aligned. See our ",(0,i.kt)("a",{parentName:"p",href:"https://engineering.linkedin.com/blog/2020/open-sourcing-datahub--linkedins-metadata-search-and-discovery-p"},"blog post")," for more details."),(0,i.kt)("h2",{id:"how-are-the-ingestion-etl-processes-scheduled-at-linkedin"},"How are the ingestion ETL processes scheduled at LinkedIn?"),(0,i.kt)("p",null,"It varies depending on the data platform. HDFS, MySQL, Oracle, Teradata, and LDAP are scheduled on a daily basis. We rely on real-time pushs to ingest from sveral data platforms such as Hive, Presto, Kafka, Pinot, ",(0,i.kt)("a",{parentName:"p",href:"https://engineering.linkedin.com/espresso/introducing-espresso-linkedins-hot-new-distributed-document-store"},"Espresso"),", ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/linkedin/ambry"},"Ambry"),", ",(0,i.kt)("a",{parentName:"p",href:"https://engineering.linkedin.com/search/did-you-mean-galene"},"Galene"),", ",(0,i.kt)("a",{parentName:"p",href:"https://engineering.linkedin.com/blog/2017/02/building-venice-with-apache-helix"},"Venice"),", and more."),(0,i.kt)("h2",{id:"what-are-the-options-for-the-kafka-key-for-mce-mae-and-failedmce-topic"},"What are the options for the Kafka Key for MCE, MAE and FailedMCE Topic?"),(0,i.kt)("p",null,"URN is the only sensible option to ensure events for the same entity land in the same parition and get processed in the chronological order."),(0,i.kt)("h2",{id:"how-is-the-data-quality-of-the-ingestion-messages-handled"},"How is the Data Quality of the ingestion messages handled?"),(0,i.kt)("p",null,"In addition to leverage the Kafka schema validation to ensure the MXEs output from metadata producer, we also actively monitor the ingestion streaming pipeline on the snapshot level with status."),(0,i.kt)("h2",{id:"can-you-give-a-high-level-overview-about-how-data-governance-is-handled-what-privacygovernance-use-cases-are-supported-in-linkedin-through-datahub"},"Can you give a high level overview about how Data Governance is handled? What privacy/governance use cases are supported in LinkedIn through DataHub?"),(0,i.kt)("p",null,"This talk (",(0,i.kt)("a",{parentName:"p",href:"https://www.slideshare.net/ShirshankaDas/taming-the-everevolving-compliance-beast-lessons-learnt-at-linkedin-strata-nyc-2017"},"slides"),", ",(0,i.kt)("a",{parentName:"p",href:"https://www.youtube.com/watch?v=O1DI0fuY8PM"},"video"),") describes the role of metadata (DataHub) in the data governance/privacy space at LinkedIn. Field-level, dataset-level classification, governed data movement, automated data deletion, data export etc. are the supported use cases. We have plans to open source some of the compliance capabilities, listed as part of our ",(0,i.kt)("a",{parentName:"p",href:"/docs/roadmap"},"roadmap"),"."),(0,i.kt)("h2",{id:"when-using-kafka-and-confluent-schema-registry-does-datahub-support-multiple-schemas-for-the-same-topic"},"When using Kafka and Confluent Schema Registry, does DataHub support multiple schemas for the same topic?"),(0,i.kt)("p",null,"You can ",(0,i.kt)("a",{parentName:"p",href:"https://docs.confluent.io/current/schema-registry/develop/api.html#compatibility"},"configure")," compatibility level per topic at Confluent Schema Registry. The default being used is \u201cBackward\u201d. So, you\u2019re only allowed to make backward compatible changes on the topic schema. You can also change this configuration and flex compatibility check. However, as a best practice, we would suggest not doing backward incompatible changes on the topic schema because this will fail your old metadata producers\u2019 flows. Instead, you might consider creating a new Kafka topic (new version)."),(0,i.kt)("h2",{id:"how-do-we-better-document-and-map-transformations-within-an-etl-process-how-do-we-create-institutional-knowledge-and-processes-to-help-create-a-paradigm-for-tribal-knowledge"},"How do we better document and map transformations within an ETL process? How do we create institutional knowledge and processes to help create a paradigm for tribal knowledge?"),(0,i.kt)("p",null,"We plan to add \u201cfine-grain lineage\u201d in the near future, which should cover the transformation documentation. DataHub currently has a simple \u201cDocs\u201d feature that allows capturing of tribal knowledge. We also plan to expand it significantly going forward."),(0,i.kt)("h2",{id:"how-do-we-advance-the-product-from-a-data-catalog-browser-to-a-data-collaboration-environment-like-alation"},"How do we advance the product from a Data Catalog Browser to a Data Collaboration environment like Alation?"),(0,i.kt)("p",null,"We are adding some \u201csocial features\u201d and documentation captures to DataHub. However, we do welcome the community to contribute in this area."),(0,i.kt)("h2",{id:"can-you-share-how-the-catalog-looks-in-linkedin-production"},"Can you share how the catalog looks in LinkedIn production?"),(0,i.kt)("p",null,"It\u2019s very similar to what you see on the community version. We have added screenshots of the internal version of the catalog in our ",(0,i.kt)("a",{parentName:"p",href:"https://engineering.linkedin.com/blog/2019/data-hub"},"blog post"),"."),(0,i.kt)("h2",{id:"does-the-roadmap-have-provision-for-capturing-the-data-quality-information-of-the-dataset"},"Does the roadmap have provision for capturing the Data Quality Information of the Dataset?"),(0,i.kt)("p",null,"We\u2019re working on a similar ",(0,i.kt)("a",{parentName:"p",href:"https://engineering.linkedin.com/blog/2020/data-sentinel-automating-data-validation"},"feature")," internally. Will evaluate and update the roadmap once we have a better idea of the timeline."),(0,i.kt)("h2",{id:"is-datahub-capturingshowing-column-level-constraints-set-at-table-definition"},"Is DataHub capturing/showing column level ",(0,i.kt)("a",{parentName:"h2",href:"https://www.w3schools.com/sql/sql_constraints.asp"},"constraints")," set at table definition?"),(0,i.kt)("p",null,"The ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/linkedin/datahub/blob/master/metadata-models/src/main/pegasus/com/linkedin/schema/SchemaField.pdl"},"SchemaField")," model currently does not capture any property/field corresponding to constraints defined in the table definition. However, it should be fairly easy to extend the model to support that if needed."),(0,i.kt)("h2",{id:"how-does-datahub-manage-extracting-metadata-from-stores-residing-in-different-security-zones"},"How does DataHub manage extracting metadata from stores residing in different security zones?"),(0,i.kt)("p",null,"MCE is the ideal way to push metadata from different security zones, assuming there is a common Kafka infrastructure that aggregates the events from various security zones."),(0,i.kt)("h2",{id:"what-all-data-stores-does-datahub-backend-support-presently"},"What all data stores does DataHub backend support presently?"),(0,i.kt)("p",null,"Currently, DataHub supports all major database providers that are supported by Ebean as the document store i.e. Oracle, Postgres, MySQL, H2. We also support ",(0,i.kt)("a",{parentName:"p",href:"https://engineering.linkedin.com/espresso/introducing-espresso-linkedins-hot-new-distributed-document-store"},"Espresso"),", which is LinkedIn's proprietary document store. Other than that, we support Elasticsearch and Neo4j for search and graph use cases, respectively. However, as data stores in the backend are all abstracted and accessed through DAOs, you should be able to easily support other data stores by plugging in your own DAO implementations. Please refer to ",(0,i.kt)("a",{parentName:"p",href:"/docs/architecture/metadata-serving"},"Metadata Serving")," for more details."),(0,i.kt)("h2",{id:"for-which-stores-you-have-discovery-services"},"For which stores, you have discovery services?"),(0,i.kt)("p",null,"Supported data sources are listed ",(0,i.kt)("a",{parentName:"p",href:"/docs/metadata-ingestion"},"here"),". It's also fairly easy to add your own sources."),(0,i.kt)("h2",{id:"how-is-metadata-ingested-in-datahub-is-it-real-time"},"How is metadata ingested in DataHub? Is it real-time?"),(0,i.kt)("p",null,"You can call the ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/linkedin/rest.li"},"rest.li")," API to ingest metadata in DataHub directly instead of using Kafka event. Metadata ingestion is real-time if you're updating via rest.li API. It's near real-time in the case of Kafka events due to the asynchronous nature of Kafka processing."))}c.isMDXComponent=!0}}]);